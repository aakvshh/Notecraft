{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4fae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0df6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_note_from_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    \n",
    "    if len(parts) > 0:\n",
    "        return parts[0]\n",
    "    else:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f80980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "# Set a fixed length for all audio samples (you can adjust this as needed)\n",
    "fixed_length = 44100  # for example, 1 second at 44.1 kHz\n",
    "fft = librosa.feature.mfcc\n",
    "\n",
    "audio_samples = []\n",
    "musical_notes = []\n",
    "\n",
    "# Define a function to load and preprocess an audio file\n",
    "def preprocess_audio(audio_file_path, target_length=fixed_length):\n",
    "    audio, sr = librosa.load(audio_file_path, sr=None)\n",
    "    \n",
    "    # Ensure that all audio samples have the same length\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    elif len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    # Extract features (FFT in this case)\n",
    "    fft_result = np.fft.fft(audio)\n",
    "    magnitudes = np.abs(fft_result)\n",
    "    \n",
    "    return magnitudes\n",
    "\n",
    "folder_path = 'piano_triads'\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.wav'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Preprocess the audio and extract features\n",
    "        features = preprocess_audio(file_path)\n",
    "        \n",
    "        note = parse_note_from_filename(filename)\n",
    "        \n",
    "        audio_samples.append(features)\n",
    "        musical_notes.append(note)\n",
    "\n",
    "X = np.array(audio_samples)\n",
    "y = np.array(musical_notes)\n",
    "\n",
    "# Now, X contains the preprocessed audio features (FFT magnitudes) and y contains the corresponding musical notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a89e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_samples):\n",
    "    features = []\n",
    "    for audio in audio_samples:\n",
    "        # Compute MFCC features (you can experiment with other feature extraction methods)\n",
    "        fft_result = librosa.feature.fft(audio, sr=44100, n_mfcc=13)\n",
    "        features.append(fft_result)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8a5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = extract_features(audio_samples)\n",
    "\n",
    "# mean_magnitudes = [np.mean(feature) for feature in features]\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(mean_magnitudes)\n",
    "# plt.xlabel('Sample Index')\n",
    "# plt.ylabel('Mean Magnitude')\n",
    "# plt.title('Mean Magnitude of FFT Coefficients for Audio Samples')\n",
    "# plt.grid()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43edfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# num_samples_to_visualize = 50\n",
    "\n",
    "# for i in range(num_samples_to_visualize):\n",
    "#     plt.figure(figsize=(7, 4))\n",
    "#     plt.specgram(X[i], Fs=sr, cmap='inferno')\n",
    "\n",
    "#     filename = os.listdir(folder_path)[i]\n",
    "    \n",
    "#     plt.title(f'Spectrogram of {filename}')\n",
    "#     plt.xlabel('Time (s)')\n",
    "#     plt.ylabel('Frequency (Hz)')\n",
    "#     plt.colorbar(format='%+2.0f dB')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be14c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #amplitude vs frequency\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# num_samples_to_visualize = 50\n",
    "\n",
    "# for i in range(num_samples_to_visualize):\n",
    "#     plt.figure(figsize=(7, 4))\n",
    "    \n",
    "#     fft_result = np.fft.fft(X[i])\n",
    "    \n",
    "#     frequencies = np.fft.fftfreq(len(fft_result), 1/sr)\n",
    "#     amplitudes = np.abs(fft_result)\n",
    "\n",
    "#     plt.plot(frequencies[:len(frequencies)//2], amplitudes[:len(amplitudes)//2])  \n",
    "#     filename = os.listdir(folder_path)[i]\n",
    "    \n",
    "#     plt.title(f'Amplitude vs. Frequency of {filename}')\n",
    "#     plt.xlabel('Frequency (Hz)')\n",
    "#     plt.ylabel('Amplitude')\n",
    "#     plt.grid()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258d260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # waveform plot\n",
    "# import librosa.display\n",
    "\n",
    "# num_samples_to_visualize = 40\n",
    "\n",
    "# for i in range(num_samples_to_visualize):\n",
    "#     plt.figure(figsize=(7, 4))\n",
    "#     plt.plot(np.arange(len(X[i])) / sr, X[i])\n",
    "#     filename = os.listdir(folder_path)[i]\n",
    "\n",
    "#     plt.title(f'Waveform of Audio Sample {filename}')\n",
    "#     plt.xlabel('Time (s)')\n",
    "#     plt.ylabel('Amplitude')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab2c5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64b1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Sample labels\n",
    "# y_train = [\"C\", \"Cs\", \"D\", \"Eb\", \"E\", \"F\", \"Fs\", \"G\", \"Gs\", \"A\", \"Bb\", \"B\"]\n",
    "\n",
    "# # Initialize the OneHotEncoder\n",
    "# one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# # Reshape and fit-transform the labels\n",
    "# y_train_encoded = one_hot_encoder.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "\n",
    "# # Now, y_train_encoded contains the one-hot encoded labels\n",
    "\n",
    "# # Inverse transformation (decoding)\n",
    "# y_train_decoded = one_hot_encoder.inverse_transform(y_train_encoded)\n",
    "\n",
    "# y_test_encoded = one_hot_encoder.transform(np.array(y_test).reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9692f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Define musical note names\n",
    "NOTE_NAMES = [\"C\", \"Cs\", \"D\", \"Eb\", \"E\", \"F\", \"Fs\", \"G\", \"Gs\", \"A\", \"Bb\", \"B\"]\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "# Define a learning rate schedule function\n",
    "def lr_schedule(epoch):\n",
    "    initial_learning_rate = 0.001  # Set your initial learning rate here\n",
    "    decay = 0.95  # Set the decay rate\n",
    "    if epoch > 5:  # Adjust this condition based on when you want to start reducing the learning rate\n",
    "        return initial_learning_rate * (decay ** (epoch - 5))\n",
    "    else:\n",
    "        return initial_learning_rate\n",
    "\n",
    "# Create a learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Create a neural network model\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Input(shape=X_train[0].shape),  # Input shape is determined by the chosen feature extraction method\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation='relu'),  # Increased the number of units in the first hidden layer\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),  # Increased the number of units in the second hidden layer\n",
    "    Dense(len(np.unique(y_train_encoded)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a9b6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (346, 44100)\n",
      "X_test shape: (87, 44100)\n",
      "y_train_encoded shape: (346,)\n",
      "y_test_encoded shape: (87,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes of your data\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train_encoded shape:\", y_train_encoded.shape)\n",
    "print(\"y_test_encoded shape:\", y_test_encoded.shape)\n",
    "\n",
    "# If there's an inconsistency, check your data split\n",
    "# Ensure that you are using the same random_state in train_test_split for reproducibility.\n",
    "# Make sure that X and y are correctly matched before the split.\n",
    "# Ensure that the test_size is reasonable and doesn't result in an extreme data split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea69df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11/11 [==============================] - 4s 182ms/step - loss: 14.2181 - accuracy: 0.1358 - val_loss: 13.0858 - val_accuracy: 0.2874 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 11.1118 - accuracy: 0.3931 - val_loss: 8.0279 - val_accuracy: 0.4023 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 1s 131ms/step - loss: 9.0279 - accuracy: 0.4798 - val_loss: 5.2419 - val_accuracy: 0.5057 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 6.2676 - accuracy: 0.5549 - val_loss: 4.1314 - val_accuracy: 0.5632 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 6.1114 - accuracy: 0.6474 - val_loss: 3.1966 - val_accuracy: 0.6322 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 4.8330 - accuracy: 0.6908 - val_loss: 2.3227 - val_accuracy: 0.6322 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 3.7829 - accuracy: 0.7197 - val_loss: 2.0846 - val_accuracy: 0.6092 - lr: 9.5000e-04\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 4.1872 - accuracy: 0.6965 - val_loss: 2.2947 - val_accuracy: 0.6322 - lr: 9.0250e-04\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 4.3978 - accuracy: 0.7370 - val_loss: 2.5582 - val_accuracy: 0.6207 - lr: 8.5737e-04\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 3.0961 - accuracy: 0.8035 - val_loss: 2.7077 - val_accuracy: 0.6897 - lr: 8.1451e-04\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 3.0930 - accuracy: 0.7832 - val_loss: 1.7179 - val_accuracy: 0.7011 - lr: 7.7378e-04\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 2.5478 - accuracy: 0.8150 - val_loss: 1.3536 - val_accuracy: 0.7586 - lr: 7.3509e-04\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 1.6700 - accuracy: 0.8266 - val_loss: 1.3837 - val_accuracy: 0.7701 - lr: 6.9834e-04\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 1.7325 - accuracy: 0.8497 - val_loss: 1.5640 - val_accuracy: 0.7126 - lr: 6.6342e-04\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 1.4361 - accuracy: 0.8786 - val_loss: 1.3426 - val_accuracy: 0.7701 - lr: 6.3025e-04\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 1.1969 - accuracy: 0.8613 - val_loss: 1.0913 - val_accuracy: 0.7471 - lr: 5.9874e-04\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 1s 132ms/step - loss: 1.2147 - accuracy: 0.8786 - val_loss: 1.2578 - val_accuracy: 0.7701 - lr: 5.6880e-04\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 1.3189 - accuracy: 0.9075 - val_loss: 1.1615 - val_accuracy: 0.7931 - lr: 5.4036e-04\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.6395 - accuracy: 0.9306 - val_loss: 1.0822 - val_accuracy: 0.8046 - lr: 5.1334e-04\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 1s 121ms/step - loss: 0.7524 - accuracy: 0.9104 - val_loss: 1.0264 - val_accuracy: 0.7586 - lr: 4.8767e-04\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 1s 131ms/step - loss: 0.8137 - accuracy: 0.9162 - val_loss: 0.9208 - val_accuracy: 0.7931 - lr: 4.6329e-04\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 1s 131ms/step - loss: 0.8222 - accuracy: 0.9046 - val_loss: 0.9203 - val_accuracy: 0.8046 - lr: 4.4013e-04\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.7392 - accuracy: 0.9075 - val_loss: 1.1603 - val_accuracy: 0.8161 - lr: 4.1812e-04\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.4101 - accuracy: 0.9393 - val_loss: 1.1667 - val_accuracy: 0.7931 - lr: 3.9721e-04\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 1s 134ms/step - loss: 0.6149 - accuracy: 0.9075 - val_loss: 1.2307 - val_accuracy: 0.7816 - lr: 3.7735e-04\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.6055 - accuracy: 0.9335 - val_loss: 1.2319 - val_accuracy: 0.7816 - lr: 3.5849e-04\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.3290 - accuracy: 0.9566 - val_loss: 1.1812 - val_accuracy: 0.7816 - lr: 3.4056e-04\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 1s 131ms/step - loss: 0.4792 - accuracy: 0.9393 - val_loss: 1.1031 - val_accuracy: 0.7701 - lr: 3.2353e-04\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.4101 - accuracy: 0.9566 - val_loss: 1.1068 - val_accuracy: 0.7931 - lr: 3.0736e-04\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 1s 134ms/step - loss: 0.4603 - accuracy: 0.9566 - val_loss: 1.1190 - val_accuracy: 0.8046 - lr: 2.9199e-04\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 2s 138ms/step - loss: 0.3541 - accuracy: 0.9480 - val_loss: 1.1768 - val_accuracy: 0.8046 - lr: 2.7739e-04\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 1s 128ms/step - loss: 0.3106 - accuracy: 0.9566 - val_loss: 1.1010 - val_accuracy: 0.8276 - lr: 2.6352e-04\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.4513 - accuracy: 0.9538 - val_loss: 1.0403 - val_accuracy: 0.8391 - lr: 2.5034e-04\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.3719 - accuracy: 0.9422 - val_loss: 1.0715 - val_accuracy: 0.8276 - lr: 2.3783e-04\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.3188 - accuracy: 0.9653 - val_loss: 1.0652 - val_accuracy: 0.8391 - lr: 2.2594e-04\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 1s 120ms/step - loss: 0.1981 - accuracy: 0.9653 - val_loss: 1.0689 - val_accuracy: 0.8276 - lr: 2.1464e-04\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.1767 - accuracy: 0.9566 - val_loss: 1.0775 - val_accuracy: 0.8276 - lr: 2.0391e-04\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.3692 - accuracy: 0.9624 - val_loss: 1.0841 - val_accuracy: 0.8161 - lr: 1.9371e-04\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.1222 - accuracy: 0.9827 - val_loss: 1.0844 - val_accuracy: 0.8276 - lr: 1.8403e-04\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.1754 - accuracy: 0.9595 - val_loss: 1.0679 - val_accuracy: 0.8391 - lr: 1.7482e-04\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 1s 119ms/step - loss: 0.1620 - accuracy: 0.9653 - val_loss: 1.1027 - val_accuracy: 0.8161 - lr: 1.6608e-04\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.1729 - accuracy: 0.9711 - val_loss: 1.1612 - val_accuracy: 0.7931 - lr: 1.5778e-04\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 1s 131ms/step - loss: 0.2042 - accuracy: 0.9595 - val_loss: 1.2038 - val_accuracy: 0.7931 - lr: 1.4989e-04\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 1s 131ms/step - loss: 0.0826 - accuracy: 0.9827 - val_loss: 1.2773 - val_accuracy: 0.7701 - lr: 1.4240e-04\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 2s 136ms/step - loss: 0.1790 - accuracy: 0.9711 - val_loss: 1.3167 - val_accuracy: 0.7586 - lr: 1.3528e-04\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 1s 137ms/step - loss: 0.0893 - accuracy: 0.9827 - val_loss: 1.3312 - val_accuracy: 0.7586 - lr: 1.2851e-04\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 1s 125ms/step - loss: 0.2338 - accuracy: 0.9653 - val_loss: 1.3799 - val_accuracy: 0.7586 - lr: 1.2209e-04\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 1s 129ms/step - loss: 0.1568 - accuracy: 0.9769 - val_loss: 1.4532 - val_accuracy: 0.7586 - lr: 1.1598e-04\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.1905 - accuracy: 0.9653 - val_loss: 1.5145 - val_accuracy: 0.7586 - lr: 1.1018e-04\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 1s 128ms/step - loss: 0.1638 - accuracy: 0.9798 - val_loss: 1.5641 - val_accuracy: 0.7701 - lr: 1.0467e-04\n"
     ]
    }
   ],
   "source": [
    "# Train your model using X_train_augmented\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=50, batch_size=32,\n",
    "                    validation_data=(X_test, y_test_encoded),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e1ce19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 13ms/step - loss: 1.5641 - accuracy: 0.7701\n",
      "Test Loss: 1.5641, Test Accuracy: 0.7701\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using encoded labels\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3a85fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ea151d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install matplotlib scipy plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4ff9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_fft(p, xf, fs, notes):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(\"Frequency Spectrum\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.plot(xf, p)\n",
    "\n",
    "    for note in notes:\n",
    "        plt.annotate(note[1], (note[0] + 10, note[2]), fontsize=12, ha='center', va='bottom')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def extract_sample(audio, frame_number, frame_offset):\n",
    "    end = frame_number * frame_offset\n",
    "    begin = int(end - FFT_WINDOW_SIZE)\n",
    "\n",
    "    if end == 0:\n",
    "        return np.zeros((np.abs(begin)), dtype=float)\n",
    "    elif begin < 0:\n",
    "        return np.concatenate([np.zeros((np.abs(begin)), dtype=float), audio[0:end]])\n",
    "    else:\n",
    "        return audio[begin:end]\n",
    "\n",
    "def find_top_notes(fft, num, xf):\n",
    "    if np.max(fft.real) < 0.001:\n",
    "        return []\n",
    "\n",
    "    lst = [x for x in enumerate(fft.real)]\n",
    "    lst = sorted(lst, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    idx = 0\n",
    "    found = []\n",
    "    found_note = set()\n",
    "    while (idx < len(lst)) and (len(found) < num):\n",
    "        f = xf[lst[idx][0]]\n",
    "        y = lst[idx][1]\n",
    "        n = freq_to_number(f)\n",
    "        n0 = int(round(n))\n",
    "        name = note_name(n0)\n",
    "\n",
    "        if name not in found_note:\n",
    "            found_note.add(name)\n",
    "            s = [f, note_name(n0), y]\n",
    "            found.append(s)\n",
    "        idx += 1\n",
    "\n",
    "    return found\n",
    "\n",
    "def freq_to_number(f): return 69 + 12 * np.log2(f / 440.0)\n",
    "def note_name(n): return NOTE_NAMES[n % 12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c072d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_fft(magnitudes, sample_rate, notes, xlim=None):\n",
    "#     fft_values = np.fft.fftfreq(len(magnitudes), 1.0 / sample_rate)\n",
    "# #\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.title(\"Frequency Spectrum\")\n",
    "#     plt.xlabel(\"Frequency (Hz)\")\n",
    "#     plt.ylabel(\"Magnitude\")\n",
    "#     plt.plot(fft_values, magnitudes)\n",
    "    \n",
    "#     if xlim:\n",
    "#         plt.xlim(xlim)\n",
    "    \n",
    "#     for note in notes:\n",
    "#         plt.annotate(note[1], (note[0] + 10, note[2]), fontsize=12, ha='center', va='bottom')\n",
    "    \n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "\n",
    "# # Example: Replace this with your actual audio_samples and sample_rate\n",
    "# # Generate random audio data for demonstration purposes\n",
    "# sample_rate = 44100\n",
    "# duration = 5  # 5 seconds of audio\n",
    "# num_samples = int(sample_rate * duration)\n",
    "# audio_samples = np.random.randn(num_samples)\n",
    "\n",
    "# # Extract features and notes (replace with your actual audio and notes)\n",
    "# features = extract_features([audio_samples])\n",
    "# num_notes = 5  # Adjust the number of top notes you want to display\n",
    "\n",
    "# for feature in features:\n",
    "#     notes = find_top_notes(feature, num_notes, fft_values)  # Replace with your notes\n",
    "#     plot_fft(feature, sample_rate, notes, xlim=(0, 1000))  # Adjust frequency range if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d186a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"mfcc_model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2302c10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step\n",
      "Predicted Note: G\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define musical note names\n",
    "NOTE_NAMES = [\"C\", \"Cs\", \"D\", \"Eb\", \"E\", \"F\", \"Fs\", \"G\", \"Gs\", \"A\", \"Bb\", \"B\"]\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model('trial1')  # Replace 'your_model_path' with the actual path to your trained model\n",
    "\n",
    "def extract_features(audio):\n",
    "    # Applying FFT\n",
    "    fft_result = np.fft.fft(audio)\n",
    "    magnitudes = np.abs(fft_result)\n",
    "    return magnitudes\n",
    "\n",
    "def predict_note(audio_file_path, model):\n",
    "    audio, _ = librosa.load(audio_file_path, sr=None)\n",
    "    features = extract_features(audio)\n",
    "    \n",
    "    if len(features) < X_train.shape[1]:\n",
    "        features = np.pad(features, (0, X_train.shape[1] - len(features)))\n",
    "    elif len(features) > X_train.shape[1]:\n",
    "        features = features[:X_train.shape[1]]\n",
    "    \n",
    "    features = np.array([features])  \n",
    "    \n",
    "    prediction = model.predict(features)\n",
    "    \n",
    "    predicted_note_index = np.argmax(prediction)\n",
    "    predicted_note = NOTE_NAMES[predicted_note_index]\n",
    "    \n",
    "    return predicted_note\n",
    "\n",
    "audio_file_path = '88-piano_new/G#_1.wav'\n",
    "predicted_note = predict_note(audio_file_path, model)\n",
    "print(f\"Predicted Note: {predicted_note}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa826e9",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61add361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
