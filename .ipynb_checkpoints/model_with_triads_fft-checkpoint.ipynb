{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4fae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0df6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_note_from_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    \n",
    "    if len(parts) > 0:\n",
    "        return parts[0]\n",
    "    else:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f80980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'piano_triads'\n",
    "\n",
    "# Set a fixed length for all audio samples (you can adjust this as needed)\n",
    "fixed_length = 44100  # for example, 1 second at 44.1 kHz\n",
    "\n",
    "audio_samples = []\n",
    "musical_notes = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.wav'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # loading audio samples\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        # Ensure that all audio samples have the same length\n",
    "        if len(audio) < fixed_length:\n",
    "            audio = np.pad(audio, (0, fixed_length - len(audio)))\n",
    "        elif len(audio) > fixed_length:\n",
    "            audio = audio[:fixed_length]\n",
    "        \n",
    "        note = parse_note_from_filename(filename)\n",
    "        \n",
    "        audio_samples.append(audio)\n",
    "        musical_notes.append(note)\n",
    "\n",
    "X = np.array(audio_samples)\n",
    "y = np.array(musical_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a89e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_samples, sample_rate=44100):\n",
    "    features = []\n",
    "    for audio in audio_samples:\n",
    "        # Applying FFT\n",
    "        fft_result = np.fft.fft(audio)\n",
    "        magnitudes = np.abs(fft_result)\n",
    "        \n",
    "        features.append(magnitudes)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8a5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = extract_features(audio_samples)\n",
    "\n",
    "# mean_magnitudes = [np.mean(feature) for feature in features]\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(mean_magnitudes)\n",
    "# plt.xlabel('Sample Index')\n",
    "# plt.ylabel('Mean Magnitude')\n",
    "# plt.title('Mean Magnitude of FFT Coefficients for Audio Samples')\n",
    "# plt.grid()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43edfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# num_samples_to_visualize = 50\n",
    "\n",
    "# for i in range(num_samples_to_visualize):\n",
    "#     plt.figure(figsize=(7, 4))\n",
    "#     plt.specgram(X[i], Fs=sr, cmap='inferno')\n",
    "\n",
    "#     filename = os.listdir(folder_path)[i]\n",
    "    \n",
    "#     plt.title(f'Spectrogram of {filename}')\n",
    "#     plt.xlabel('Time (s)')\n",
    "#     plt.ylabel('Frequency (Hz)')\n",
    "#     plt.colorbar(format='%+2.0f dB')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be14c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #amplitude vs frequency\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# num_samples_to_visualize = 50\n",
    "\n",
    "# for i in range(num_samples_to_visualize):\n",
    "#     plt.figure(figsize=(7, 4))\n",
    "    \n",
    "#     fft_result = np.fft.fft(X[i])\n",
    "    \n",
    "#     frequencies = np.fft.fftfreq(len(fft_result), 1/sr)\n",
    "#     amplitudes = np.abs(fft_result)\n",
    "\n",
    "#     plt.plot(frequencies[:len(frequencies)//2], amplitudes[:len(amplitudes)//2])  \n",
    "#     filename = os.listdir(folder_path)[i]\n",
    "    \n",
    "#     plt.title(f'Amplitude vs. Frequency of {filename}')\n",
    "#     plt.xlabel('Frequency (Hz)')\n",
    "#     plt.ylabel('Amplitude')\n",
    "#     plt.grid()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258d260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # waveform plot\n",
    "# import librosa.display\n",
    "\n",
    "# num_samples_to_visualize = 40\n",
    "\n",
    "# for i in range(num_samples_to_visualize):\n",
    "#     plt.figure(figsize=(7, 4))\n",
    "#     plt.plot(np.arange(len(X[i])) / sr, X[i])\n",
    "#     filename = os.listdir(folder_path)[i]\n",
    "\n",
    "#     plt.title(f'Waveform of Audio Sample {filename}')\n",
    "#     plt.xlabel('Time (s)')\n",
    "#     plt.ylabel('Amplitude')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab2c5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64b1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9692f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Define musical note names\n",
    "NOTE_NAMES = [\"C\", \"Cs\", \"D\", \"Eb\", \"E\", \"F\", \"Fs\", \"G\", \"Gs\", \"A\", \"Bb\", \"B\"]\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "# Define a learning rate schedule function\n",
    "def lr_schedule(epoch):\n",
    "    initial_learning_rate = 0.001  # Set your initial learning rate here\n",
    "    decay = 0.95  # Set the decay rate\n",
    "    if epoch > 5:  # Adjust this condition based on when you want to start reducing the learning rate\n",
    "        return initial_learning_rate * (decay ** (epoch - 5))\n",
    "    else:\n",
    "        return initial_learning_rate\n",
    "\n",
    "# Create a learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Create a neural network model\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train_encoded)), activation='softmax')  # Output layer with softmax for classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea69df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 188ms/step - loss: 2.5441 - accuracy: 0.1429 - val_loss: 3.0044 - val_accuracy: 0.1111 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.7779 - accuracy: 0.9000 - val_loss: 3.5408 - val_accuracy: 0.1667 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.3792 - accuracy: 0.9714 - val_loss: 4.0295 - val_accuracy: 0.2222 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.2463 - accuracy: 1.0000 - val_loss: 4.4001 - val_accuracy: 0.2778 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.1792 - accuracy: 1.0000 - val_loss: 4.6659 - val_accuracy: 0.2778 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.1342 - accuracy: 1.0000 - val_loss: 4.8841 - val_accuracy: 0.2778 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.1030 - accuracy: 1.0000 - val_loss: 5.0567 - val_accuracy: 0.3333 - lr: 9.5000e-04\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.0767 - accuracy: 1.0000 - val_loss: 5.1929 - val_accuracy: 0.3333 - lr: 9.0250e-04\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.0552 - accuracy: 1.0000 - val_loss: 5.2984 - val_accuracy: 0.3333 - lr: 8.5737e-04\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.0433 - accuracy: 1.0000 - val_loss: 5.3812 - val_accuracy: 0.3333 - lr: 8.1451e-04\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 5.4467 - val_accuracy: 0.3333 - lr: 7.7378e-04\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 5.4997 - val_accuracy: 0.3333 - lr: 7.3509e-04\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 5.5417 - val_accuracy: 0.3333 - lr: 6.9834e-04\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 5.5754 - val_accuracy: 0.3333 - lr: 6.6342e-04\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 5.6029 - val_accuracy: 0.3333 - lr: 6.3025e-04\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 5.6270 - val_accuracy: 0.3333 - lr: 5.9874e-04\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 5.6469 - val_accuracy: 0.3333 - lr: 5.6880e-04\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 5.6624 - val_accuracy: 0.3333 - lr: 5.4036e-04\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 5.6749 - val_accuracy: 0.3333 - lr: 5.1334e-04\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 5.6847 - val_accuracy: 0.3333 - lr: 4.8767e-04\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 5.6931 - val_accuracy: 0.3333 - lr: 4.6329e-04\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 5.7002 - val_accuracy: 0.3333 - lr: 4.4013e-04\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 5.7063 - val_accuracy: 0.3333 - lr: 4.1812e-04\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 5.7116 - val_accuracy: 0.3333 - lr: 3.9721e-04\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 5.7168 - val_accuracy: 0.3333 - lr: 3.7735e-04\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 5.7221 - val_accuracy: 0.3333 - lr: 3.5849e-04\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 5.7268 - val_accuracy: 0.3333 - lr: 3.4056e-04\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 5.7316 - val_accuracy: 0.3333 - lr: 3.2353e-04\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 5.7356 - val_accuracy: 0.3333 - lr: 3.0736e-04\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 5.7395 - val_accuracy: 0.3333 - lr: 2.9199e-04\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 5.7438 - val_accuracy: 0.3333 - lr: 2.7739e-04\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 5.7480 - val_accuracy: 0.3333 - lr: 2.6352e-04\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 5.7518 - val_accuracy: 0.3333 - lr: 2.5034e-04\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 5.7548 - val_accuracy: 0.3333 - lr: 2.3783e-04\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 5.7575 - val_accuracy: 0.3333 - lr: 2.2594e-04\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 5.7608 - val_accuracy: 0.3333 - lr: 2.1464e-04\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 5.7639 - val_accuracy: 0.3333 - lr: 2.0391e-04\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 5.7665 - val_accuracy: 0.3333 - lr: 1.9371e-04\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 5.7686 - val_accuracy: 0.3333 - lr: 1.8403e-04\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 5.7704 - val_accuracy: 0.3333 - lr: 1.7482e-04\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 5.7721 - val_accuracy: 0.3333 - lr: 1.6608e-04\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 5.7738 - val_accuracy: 0.3333 - lr: 1.5778e-04\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 5.7755 - val_accuracy: 0.3333 - lr: 1.4989e-04\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 5.7772 - val_accuracy: 0.3333 - lr: 1.4240e-04\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 5.7785 - val_accuracy: 0.3333 - lr: 1.3528e-04\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 5.7798 - val_accuracy: 0.3333 - lr: 1.2851e-04\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 5.7809 - val_accuracy: 0.3333 - lr: 1.2209e-04\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 5.7819 - val_accuracy: 0.3333 - lr: 1.1598e-04\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 5.7830 - val_accuracy: 0.3333 - lr: 1.1018e-04\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 5.7840 - val_accuracy: 0.3333 - lr: 1.0467e-04\n"
     ]
    }
   ],
   "source": [
    "# Train your model using X_train_augmented\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=50, batch_size=32,\n",
    "                    validation_data=(X_test, y_test_encoded),\n",
    "                    callbacks=[lr_scheduler])# Train your model using X_train_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e1ce19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step - loss: 5.7840 - accuracy: 0.3333\n",
      "Test Loss: 5.7840, Test Accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using encoded labels\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3a85fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 112ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ea151d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install matplotlib scipy plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4ff9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_fft(p, xf, fs, notes):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(\"Frequency Spectrum\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.plot(xf, p)\n",
    "\n",
    "    for note in notes:\n",
    "        plt.annotate(note[1], (note[0] + 10, note[2]), fontsize=12, ha='center', va='bottom')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def extract_sample(audio, frame_number, frame_offset):\n",
    "    end = frame_number * frame_offset\n",
    "    begin = int(end - FFT_WINDOW_SIZE)\n",
    "\n",
    "    if end == 0:\n",
    "        return np.zeros((np.abs(begin)), dtype=float)\n",
    "    elif begin < 0:\n",
    "        return np.concatenate([np.zeros((np.abs(begin)), dtype=float), audio[0:end]])\n",
    "    else:\n",
    "        return audio[begin:end]\n",
    "\n",
    "def find_top_notes(fft, num, xf):\n",
    "    if np.max(fft.real) < 0.001:\n",
    "        return []\n",
    "\n",
    "    lst = [x for x in enumerate(fft.real)]\n",
    "    lst = sorted(lst, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    idx = 0\n",
    "    found = []\n",
    "    found_note = set()\n",
    "    while (idx < len(lst)) and (len(found) < num):\n",
    "        f = xf[lst[idx][0]]\n",
    "        y = lst[idx][1]\n",
    "        n = freq_to_number(f)\n",
    "        n0 = int(round(n))\n",
    "        name = note_name(n0)\n",
    "\n",
    "        if name not in found_note:\n",
    "            found_note.add(name)\n",
    "            s = [f, note_name(n0), y]\n",
    "            found.append(s)\n",
    "        idx += 1\n",
    "\n",
    "    return found\n",
    "\n",
    "def freq_to_number(f): return 69 + 12 * np.log2(f / 440.0)\n",
    "def note_name(n): return NOTE_NAMES[n % 12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c072d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_fft(magnitudes, sample_rate, notes, xlim=None):\n",
    "#     fft_values = np.fft.fftfreq(len(magnitudes), 1.0 / sample_rate)\n",
    "# #\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.title(\"Frequency Spectrum\")\n",
    "#     plt.xlabel(\"Frequency (Hz)\")\n",
    "#     plt.ylabel(\"Magnitude\")\n",
    "#     plt.plot(fft_values, magnitudes)\n",
    "    \n",
    "#     if xlim:\n",
    "#         plt.xlim(xlim)\n",
    "    \n",
    "#     for note in notes:\n",
    "#         plt.annotate(note[1], (note[0] + 10, note[2]), fontsize=12, ha='center', va='bottom')\n",
    "    \n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "\n",
    "# # Example: Replace this with your actual audio_samples and sample_rate\n",
    "# # Generate random audio data for demonstration purposes\n",
    "# sample_rate = 44100\n",
    "# duration = 5  # 5 seconds of audio\n",
    "# num_samples = int(sample_rate * duration)\n",
    "# audio_samples = np.random.randn(num_samples)\n",
    "\n",
    "# # Extract features and notes (replace with your actual audio and notes)\n",
    "# features = extract_features([audio_samples])\n",
    "# num_notes = 5  # Adjust the number of top notes you want to display\n",
    "\n",
    "# for feature in features:\n",
    "#     notes = find_top_notes(feature, num_notes, fft_values)  # Replace with your notes\n",
    "#     plot_fft(feature, sample_rate, notes, xlim=(0, 1000))  # Adjust frequency range if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d186a315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trial1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trial1\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"trial1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2302c10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 81ms/step\n",
      "Predicted Note: Fs\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define musical note names\n",
    "NOTE_NAMES = [\"C\", \"Cs\", \"D\", \"Eb\", \"E\", \"F\", \"Fs\", \"G\", \"Gs\", \"A\", \"Bb\", \"B\"]\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model('trial1')  # Replace 'your_model_path' with the actual path to your trained model\n",
    "\n",
    "def extract_features(audio):\n",
    "    # Applying FFT\n",
    "    fft_result = np.fft.fft(audio)\n",
    "    magnitudes = np.abs(fft_result)\n",
    "    return magnitudes\n",
    "\n",
    "def predict_note(audio_file_path, model):\n",
    "    # Load and process the audio file\n",
    "    audio, _ = librosa.load(audio_file_path, sr=None)\n",
    "    features = extract_features(audio)\n",
    "    \n",
    "    # Make sure the features have the same shape as the model's input shape\n",
    "    if len(features) < X_train.shape[1]:\n",
    "        features = np.pad(features, (0, X_train.shape[1] - len(features)))\n",
    "    elif len(features) > X_train.shape[1]:\n",
    "        features = features[:X_train.shape[1]]\n",
    "    \n",
    "    features = np.array([features])  # Reshape for model input\n",
    "    \n",
    "    # Make a prediction using the trained model\n",
    "    prediction = model.predict(features)\n",
    "    \n",
    "    # Convert prediction to musical note\n",
    "    predicted_note_index = np.argmax(prediction)\n",
    "    predicted_note = NOTE_NAMES[predicted_note_index]\n",
    "    \n",
    "    return predicted_note\n",
    "\n",
    "# Example usage:\n",
    "audio_file_path = 'piano_triads/C_maj_4_0.wav'  # Replace 'your_audio_file_path.wav' with the path to the user's input audio file\n",
    "predicted_note = predict_note(audio_file_path, model)\n",
    "print(f\"Predicted Note: {predicted_note}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa826e9",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61add361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
