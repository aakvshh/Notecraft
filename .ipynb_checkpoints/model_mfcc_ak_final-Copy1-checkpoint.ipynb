{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4fae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0df6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_note_from_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    \n",
    "    if len(parts) > 0:\n",
    "        return parts[0]\n",
    "    else:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f80980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "# Set a fixed length for all audio samples (you can adjust this as needed)\n",
    "fixed_length = 44100  # for example, 1 second at 44.1 kHz\n",
    "fft = librosa.feature.mfcc\n",
    "\n",
    "audio_samples = []\n",
    "musical_notes = []\n",
    "\n",
    "# Define a function to load and preprocess an audio file\n",
    "def preprocess_audio(audio_file_path, target_length=fixed_length):\n",
    "    audio, sr = librosa.load(audio_file_path, sr=None)\n",
    "    \n",
    "    # Ensure that all audio samples have the same length\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    elif len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    # Extract features (FFT in this case)\n",
    "    fft_result = np.fft.fft(audio)\n",
    "    magnitudes = np.abs(fft_result)\n",
    "    \n",
    "    return magnitudes\n",
    "\n",
    "folder_path = 'piano_triads'\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.wav'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Preprocess the audio and extract features\n",
    "        features = preprocess_audio(file_path)\n",
    "        \n",
    "        note = parse_note_from_filename(filename)\n",
    "        \n",
    "        audio_samples.append(features)\n",
    "        musical_notes.append(note)\n",
    "\n",
    "X = np.array(audio_samples)\n",
    "y = np.array(musical_notes)\n",
    "\n",
    "# Now, X contains the preprocessed audio features (FFT magnitudes) and y contains the corresponding musical notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a89e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_samples):\n",
    "    features = []\n",
    "    for audio in audio_samples:\n",
    "        # Compute MFCC features (you can experiment with other feature extraction methods)\n",
    "        fft_result = librosa.feature.fft(audio, sr=44100, n_mfcc=13)\n",
    "        features.append(fft_result)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8a5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = extract_features(audio_samples)\n",
    "\n",
    "# mean_magnitudes = [np.mean(feature) for feature in features]\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(mean_magnitudes)\n",
    "# plt.xlabel('Sample Index')\n",
    "# plt.ylabel('Mean Magnitude')\n",
    "# plt.title('Mean Magnitude of FFT Coefficients for Audio Samples')\n",
    "# plt.grid()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43edfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# num_samples_to_visualize = 50\n",
    "\n",
    "# for i in range(num_samples_to_visualize):\n",
    "#     plt.figure(figsize=(7, 4))\n",
    "#     plt.specgram(X[i], Fs=sr, cmap='inferno')\n",
    "\n",
    "#     filename = os.listdir(folder_path)[i]\n",
    "    \n",
    "#     plt.title(f'Spectrogram of {filename}')\n",
    "#     plt.xlabel('Time (s)')\n",
    "#     plt.ylabel('Frequency (Hz)')\n",
    "#     plt.colorbar(format='%+2.0f dB')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be14c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #amplitude vs frequency\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# num_samples_to_visualize = 50\n",
    "\n",
    "# for i in range(num_samples_to_visualize):\n",
    "#     plt.figure(figsize=(7, 4))\n",
    "    \n",
    "#     fft_result = np.fft.fft(X[i])\n",
    "    \n",
    "#     frequencies = np.fft.fftfreq(len(fft_result), 1/sr)\n",
    "#     amplitudes = np.abs(fft_result)\n",
    "\n",
    "#     plt.plot(frequencies[:len(frequencies)//2], amplitudes[:len(amplitudes)//2])  \n",
    "#     filename = os.listdir(folder_path)[i]\n",
    "    \n",
    "#     plt.title(f'Amplitude vs. Frequency of {filename}')\n",
    "#     plt.xlabel('Frequency (Hz)')\n",
    "#     plt.ylabel('Amplitude')\n",
    "#     plt.grid()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258d260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # waveform plot\n",
    "# import librosa.display\n",
    "\n",
    "# num_samples_to_visualize = 40\n",
    "\n",
    "# for i in range(num_samples_to_visualize):\n",
    "#     plt.figure(figsize=(7, 4))\n",
    "#     plt.plot(np.arange(len(X[i])) / sr, X[i])\n",
    "#     filename = os.listdir(folder_path)[i]\n",
    "\n",
    "#     plt.title(f'Waveform of Audio Sample {filename}')\n",
    "#     plt.xlabel('Time (s)')\n",
    "#     plt.ylabel('Amplitude')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab2c5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64b1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Sample labels\n",
    "# y_train = [\"C\", \"Cs\", \"D\", \"Eb\", \"E\", \"F\", \"Fs\", \"G\", \"Gs\", \"A\", \"Bb\", \"B\"]\n",
    "\n",
    "# # Initialize the OneHotEncoder\n",
    "# one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# # Reshape and fit-transform the labels\n",
    "# y_train_encoded = one_hot_encoder.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "\n",
    "# # Now, y_train_encoded contains the one-hot encoded labels\n",
    "\n",
    "# # Inverse transformation (decoding)\n",
    "# y_train_decoded = one_hot_encoder.inverse_transform(y_train_encoded)\n",
    "\n",
    "# y_test_encoded = one_hot_encoder.transform(np.array(y_test).reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9692f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "# Define musical note names\n",
    "NOTE_NAMES = [\"C\", \"Cs\", \"D\", \"Eb\", \"E\", \"F\", \"Fs\", \"G\", \"Gs\", \"A\", \"Bb\", \"B\"]\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "# Define a learning rate schedule function\n",
    "def lr_schedule(epoch):\n",
    "    initial_learning_rate = 0.001  # Set your initial learning rate here\n",
    "    decay = 0.95  # Set the decay rate\n",
    "    if epoch > 5:  # Adjust this condition based on when you want to start reducing the learning rate\n",
    "        return initial_learning_rate * (decay ** (epoch - 5))\n",
    "    else:\n",
    "        return initial_learning_rate\n",
    "\n",
    "# Create a learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Create a neural network model\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Input(shape=X_train[0].shape),  # Input shape is determined by the chosen feature extraction method\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation='relu'),  # Increased the number of units in the first hidden layer\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),  # Increased the number of units in the second hidden layer\n",
    "    Dense(len(np.unique(y_train_encoded)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c5a9b6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (346, 44100)\n",
      "X_test shape: (87, 44100)\n",
      "y_train_encoded shape: (346,)\n",
      "y_test_encoded shape: (87,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes of your data\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train_encoded shape:\", y_train_encoded.shape)\n",
    "print(\"y_test_encoded shape:\", y_test_encoded.shape)\n",
    "\n",
    "# If there's an inconsistency, check your data split\n",
    "# Ensure that you are using the same random_state in train_test_split for reproducibility.\n",
    "# Make sure that X and y are correctly matched before the split.\n",
    "# Ensure that the test_size is reasonable and doesn't result in an extreme data split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ea69df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11/11 [==============================] - 5s 289ms/step - loss: 12.7407 - accuracy: 0.1676 - val_loss: 5.7418 - val_accuracy: 0.3908 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 3s 270ms/step - loss: 10.6271 - accuracy: 0.3642 - val_loss: 6.2176 - val_accuracy: 0.4828 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 3s 289ms/step - loss: 8.8002 - accuracy: 0.5087 - val_loss: 4.7079 - val_accuracy: 0.5517 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 7.0245 - accuracy: 0.5925 - val_loss: 3.6536 - val_accuracy: 0.5747 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 3s 275ms/step - loss: 5.0917 - accuracy: 0.6618 - val_loss: 3.2281 - val_accuracy: 0.5862 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 3s 264ms/step - loss: 4.6828 - accuracy: 0.6821 - val_loss: 2.8021 - val_accuracy: 0.6437 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 4.0583 - accuracy: 0.6763 - val_loss: 2.4655 - val_accuracy: 0.6552 - lr: 9.5000e-04\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 3s 277ms/step - loss: 3.7403 - accuracy: 0.7312 - val_loss: 1.9271 - val_accuracy: 0.6782 - lr: 9.0250e-04\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 3s 274ms/step - loss: 2.6955 - accuracy: 0.7688 - val_loss: 2.1393 - val_accuracy: 0.6782 - lr: 8.5737e-04\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 3s 270ms/step - loss: 2.5176 - accuracy: 0.8064 - val_loss: 1.9394 - val_accuracy: 0.6092 - lr: 8.1451e-04\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 3s 267ms/step - loss: 2.3166 - accuracy: 0.8179 - val_loss: 2.0698 - val_accuracy: 0.6782 - lr: 7.7378e-04\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 3s 301ms/step - loss: 2.6930 - accuracy: 0.8150 - val_loss: 1.4818 - val_accuracy: 0.7471 - lr: 7.3509e-04\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 3s 289ms/step - loss: 2.1110 - accuracy: 0.8208 - val_loss: 1.1114 - val_accuracy: 0.7701 - lr: 6.9834e-04\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 3s 285ms/step - loss: 1.5232 - accuracy: 0.8786 - val_loss: 1.1556 - val_accuracy: 0.7816 - lr: 6.6342e-04\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 3s 287ms/step - loss: 2.2570 - accuracy: 0.8613 - val_loss: 1.1359 - val_accuracy: 0.7471 - lr: 6.3025e-04\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 3s 278ms/step - loss: 1.8370 - accuracy: 0.8410 - val_loss: 1.3758 - val_accuracy: 0.7471 - lr: 5.9874e-04\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 3s 268ms/step - loss: 1.1422 - accuracy: 0.8931 - val_loss: 1.1202 - val_accuracy: 0.7701 - lr: 5.6880e-04\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 3s 287ms/step - loss: 1.5637 - accuracy: 0.8902 - val_loss: 1.2077 - val_accuracy: 0.8046 - lr: 5.4036e-04\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 3s 300ms/step - loss: 1.0455 - accuracy: 0.9162 - val_loss: 1.2389 - val_accuracy: 0.7816 - lr: 5.1334e-04\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 3s 276ms/step - loss: 0.7085 - accuracy: 0.9162 - val_loss: 1.4230 - val_accuracy: 0.7586 - lr: 4.8767e-04\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 3s 276ms/step - loss: 1.0566 - accuracy: 0.8931 - val_loss: 1.3896 - val_accuracy: 0.7701 - lr: 4.6329e-04\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 3s 297ms/step - loss: 0.7759 - accuracy: 0.9364 - val_loss: 1.0314 - val_accuracy: 0.7931 - lr: 4.4013e-04\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 3s 276ms/step - loss: 0.9367 - accuracy: 0.9046 - val_loss: 0.9591 - val_accuracy: 0.7931 - lr: 4.1812e-04\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 3s 294ms/step - loss: 0.5286 - accuracy: 0.9277 - val_loss: 1.0229 - val_accuracy: 0.8046 - lr: 3.9721e-04\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 3s 295ms/step - loss: 0.5518 - accuracy: 0.9480 - val_loss: 1.0925 - val_accuracy: 0.8046 - lr: 3.7735e-04\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.4838 - accuracy: 0.9277 - val_loss: 1.1794 - val_accuracy: 0.8046 - lr: 3.5849e-04\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 3s 291ms/step - loss: 0.6349 - accuracy: 0.9364 - val_loss: 1.2647 - val_accuracy: 0.8046 - lr: 3.4056e-04\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 3s 277ms/step - loss: 0.2948 - accuracy: 0.9624 - val_loss: 1.3220 - val_accuracy: 0.8391 - lr: 3.2353e-04\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 3s 291ms/step - loss: 0.4397 - accuracy: 0.9306 - val_loss: 1.3892 - val_accuracy: 0.8391 - lr: 3.0736e-04\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 3s 289ms/step - loss: 0.6264 - accuracy: 0.9191 - val_loss: 1.2478 - val_accuracy: 0.8161 - lr: 2.9199e-04\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 3s 276ms/step - loss: 0.6438 - accuracy: 0.9249 - val_loss: 1.4396 - val_accuracy: 0.8161 - lr: 2.7739e-04\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 3s 296ms/step - loss: 0.4314 - accuracy: 0.9335 - val_loss: 1.6639 - val_accuracy: 0.8276 - lr: 2.6352e-04\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 3s 275ms/step - loss: 0.3957 - accuracy: 0.9422 - val_loss: 1.7938 - val_accuracy: 0.8161 - lr: 2.5034e-04\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 3s 278ms/step - loss: 0.2183 - accuracy: 0.9538 - val_loss: 1.7920 - val_accuracy: 0.8161 - lr: 2.3783e-04\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 3s 279ms/step - loss: 0.4851 - accuracy: 0.9451 - val_loss: 1.6828 - val_accuracy: 0.8046 - lr: 2.2594e-04\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 3s 288ms/step - loss: 0.2210 - accuracy: 0.9711 - val_loss: 1.5828 - val_accuracy: 0.8276 - lr: 2.1464e-04\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 3s 292ms/step - loss: 0.2277 - accuracy: 0.9624 - val_loss: 1.5283 - val_accuracy: 0.8161 - lr: 2.0391e-04\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 3s 309ms/step - loss: 0.2221 - accuracy: 0.9566 - val_loss: 1.4935 - val_accuracy: 0.8161 - lr: 1.9371e-04\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 3s 278ms/step - loss: 0.1527 - accuracy: 0.9682 - val_loss: 1.4562 - val_accuracy: 0.8276 - lr: 1.8403e-04\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 3s 296ms/step - loss: 0.1454 - accuracy: 0.9740 - val_loss: 1.4088 - val_accuracy: 0.8161 - lr: 1.7482e-04\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 3s 273ms/step - loss: 0.2230 - accuracy: 0.9538 - val_loss: 1.4088 - val_accuracy: 0.8161 - lr: 1.6608e-04\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 3s 265ms/step - loss: 0.1945 - accuracy: 0.9538 - val_loss: 1.3971 - val_accuracy: 0.8046 - lr: 1.5778e-04\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 3s 267ms/step - loss: 0.3445 - accuracy: 0.9451 - val_loss: 1.4027 - val_accuracy: 0.8161 - lr: 1.4989e-04\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 3s 257ms/step - loss: 0.1385 - accuracy: 0.9682 - val_loss: 1.4155 - val_accuracy: 0.8046 - lr: 1.4240e-04\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 3s 272ms/step - loss: 0.1706 - accuracy: 0.9682 - val_loss: 1.3522 - val_accuracy: 0.8046 - lr: 1.3528e-04\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 3s 290ms/step - loss: 0.1751 - accuracy: 0.9711 - val_loss: 1.3474 - val_accuracy: 0.8046 - lr: 1.2851e-04\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 3s 272ms/step - loss: 0.1011 - accuracy: 0.9855 - val_loss: 1.3516 - val_accuracy: 0.8046 - lr: 1.2209e-04\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 3s 280ms/step - loss: 0.1356 - accuracy: 0.9740 - val_loss: 1.4018 - val_accuracy: 0.8046 - lr: 1.1598e-04\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.1682 - accuracy: 0.9711 - val_loss: 1.4214 - val_accuracy: 0.7931 - lr: 1.1018e-04\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.1222 - accuracy: 0.9740 - val_loss: 1.4122 - val_accuracy: 0.7931 - lr: 1.0467e-04\n"
     ]
    }
   ],
   "source": [
    "# Train your model using X_train_augmented\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=50, batch_size=32,\n",
    "                    validation_data=(X_test, y_test_encoded),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e1ce19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 23ms/step - loss: 1.2926 - accuracy: 0.8046\n",
      "Test Loss: 1.2926, Test Accuracy: 0.8046\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using encoded labels\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3a85fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ea151d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install matplotlib scipy plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4ff9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_fft(p, xf, fs, notes):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(\"Frequency Spectrum\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.plot(xf, p)\n",
    "\n",
    "    for note in notes:\n",
    "        plt.annotate(note[1], (note[0] + 10, note[2]), fontsize=12, ha='center', va='bottom')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def extract_sample(audio, frame_number, frame_offset):\n",
    "    end = frame_number * frame_offset\n",
    "    begin = int(end - FFT_WINDOW_SIZE)\n",
    "\n",
    "    if end == 0:\n",
    "        return np.zeros((np.abs(begin)), dtype=float)\n",
    "    elif begin < 0:\n",
    "        return np.concatenate([np.zeros((np.abs(begin)), dtype=float), audio[0:end]])\n",
    "    else:\n",
    "        return audio[begin:end]\n",
    "\n",
    "def find_top_notes(fft, num, xf):\n",
    "    if np.max(fft.real) < 0.001:\n",
    "        return []\n",
    "\n",
    "    lst = [x for x in enumerate(fft.real)]\n",
    "    lst = sorted(lst, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    idx = 0\n",
    "    found = []\n",
    "    found_note = set()\n",
    "    while (idx < len(lst)) and (len(found) < num):\n",
    "        f = xf[lst[idx][0]]\n",
    "        y = lst[idx][1]\n",
    "        n = freq_to_number(f)\n",
    "        n0 = int(round(n))\n",
    "        name = note_name(n0)\n",
    "\n",
    "        if name not in found_note:\n",
    "            found_note.add(name)\n",
    "            s = [f, note_name(n0), y]\n",
    "            found.append(s)\n",
    "        idx += 1\n",
    "\n",
    "    return found\n",
    "\n",
    "def freq_to_number(f): return 69 + 12 * np.log2(f / 440.0)\n",
    "def note_name(n): return NOTE_NAMES[n % 12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c072d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_fft(magnitudes, sample_rate, notes, xlim=None):\n",
    "#     fft_values = np.fft.fftfreq(len(magnitudes), 1.0 / sample_rate)\n",
    "# #\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.title(\"Frequency Spectrum\")\n",
    "#     plt.xlabel(\"Frequency (Hz)\")\n",
    "#     plt.ylabel(\"Magnitude\")\n",
    "#     plt.plot(fft_values, magnitudes)\n",
    "    \n",
    "#     if xlim:\n",
    "#         plt.xlim(xlim)\n",
    "    \n",
    "#     for note in notes:\n",
    "#         plt.annotate(note[1], (note[0] + 10, note[2]), fontsize=12, ha='center', va='bottom')\n",
    "    \n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "\n",
    "# # Example: Replace this with your actual audio_samples and sample_rate\n",
    "# # Generate random audio data for demonstration purposes\n",
    "# sample_rate = 44100\n",
    "# duration = 5  # 5 seconds of audio\n",
    "# num_samples = int(sample_rate * duration)\n",
    "# audio_samples = np.random.randn(num_samples)\n",
    "\n",
    "# # Extract features and notes (replace with your actual audio and notes)\n",
    "# features = extract_features([audio_samples])\n",
    "# num_notes = 5  # Adjust the number of top notes you want to display\n",
    "\n",
    "# for feature in features:\n",
    "#     notes = find_top_notes(feature, num_notes, fft_values)  # Replace with your notes\n",
    "#     plot_fft(feature, sample_rate, notes, xlim=(0, 1000))  # Adjust frequency range if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d186a315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mfcc_model2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mfcc_model2\\assets\n"
     ]
    }
   ],
   "source": [
    "# model.save(\"mfcc_model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2302c10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 128ms/step\n",
      "Predicted Note: Bb\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define musical note names\n",
    "NOTE_NAMES = [\"C\", \"Cs\", \"D\", \"Eb\", \"E\", \"F\", \"Fs\", \"G\", \"Gs\", \"A\", \"Bb\", \"B\"]\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model('mfcc_model2')  # Replace 'your_model_path' with the actual path to your trained model\n",
    "\n",
    "def extract_features(audio):\n",
    "    # Applying FFT\n",
    "    fft_result = np.fft.fft(audio)\n",
    "    magnitudes = np.abs(fft_result)\n",
    "    return magnitudes\n",
    "\n",
    "def predict_note(audio_file_path, model):\n",
    "    audio, _ = librosa.load(audio_file_path, sr=None)\n",
    "    features = extract_features(audio)\n",
    "    \n",
    "    if len(features) < X_train.shape[1]:\n",
    "        features = np.pad(features, (0, X_train.shape[1] - len(features)))\n",
    "    elif len(features) > X_train.shape[1]:\n",
    "        features = features[:X_train.shape[1]]\n",
    "    \n",
    "    features = np.array([features])  \n",
    "    \n",
    "    prediction = model.predict(features)\n",
    "    \n",
    "    predicted_note_index = np.argmax(prediction)\n",
    "    predicted_note = NOTE_NAMES[predicted_note_index]\n",
    "    \n",
    "    return predicted_note\n",
    "\n",
    "audio_file_path = '88-piano_new/G_7.wav'\n",
    "predicted_note = predict_note(audio_file_path, model)\n",
    "print(f\"Predicted Note: {predicted_note}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa826e9",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61add361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
