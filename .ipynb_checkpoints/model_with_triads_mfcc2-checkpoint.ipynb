{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4fae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0df6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_note_from_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    \n",
    "    if len(parts) > 0:\n",
    "        return parts[0]\n",
    "    else:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f80980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "# Set a fixed length for all audio samples (you can adjust this as needed)\n",
    "fixed_length = 44100  # for example, 1 second at 44.1 kHz\n",
    "\n",
    "audio_samples = []\n",
    "musical_notes = []\n",
    "\n",
    "# Define a function to load and preprocess an audio file\n",
    "def preprocess_audio(audio_file_path, target_length=fixed_length):\n",
    "    audio, sr = librosa.load(audio_file_path, sr=None)\n",
    "    \n",
    "    # Ensure that all audio samples have the same length\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    elif len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    # Extract features (FFT in this case)\n",
    "    fft_result = np.fft.fft(audio)\n",
    "    magnitudes = np.abs(fft_result)\n",
    "    \n",
    "    return magnitudes\n",
    "\n",
    "folder_path = 'piano_triads'\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.wav'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Preprocess the audio and extract features\n",
    "        features = preprocess_audio(file_path)\n",
    "        \n",
    "        note = parse_note_from_filename(filename)\n",
    "        \n",
    "        audio_samples.append(features)\n",
    "        musical_notes.append(note)\n",
    "\n",
    "X = np.array(audio_samples)\n",
    "y = np.array(musical_notes)\n",
    "\n",
    "# Now, X contains the preprocessed audio features (FFT magnitudes) and y contains the corresponding musical notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a89e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_samples, sample_rate=44100):\n",
    "    features = []\n",
    "    for audio in audio_samples:\n",
    "        # Applying FFT\n",
    "        fft_result = np.fft.fft(audio)\n",
    "        magnitudes = np.abs(fft_result)\n",
    "        \n",
    "        features.append(magnitudes)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8a5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = extract_features(audio_samples)\n",
    "\n",
    "# mean_magnitudes = [np.mean(feature) for feature in features]\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(mean_magnitudes)\n",
    "# plt.xlabel('Sample Index')\n",
    "# plt.ylabel('Mean Magnitude')\n",
    "# plt.title('Mean Magnitude of FFT Coefficients for Audio Samples')\n",
    "# plt.grid()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43edfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# num_samples_to_visualize = 50\n",
    "\n",
    "# for i in range(num_samples_to_visualize):\n",
    "#     plt.figure(figsize=(7, 4))\n",
    "#     plt.specgram(X[i], Fs=sr, cmap='inferno')\n",
    "\n",
    "#     filename = os.listdir(folder_path)[i]\n",
    "    \n",
    "#     plt.title(f'Spectrogram of {filename}')\n",
    "#     plt.xlabel('Time (s)')\n",
    "#     plt.ylabel('Frequency (Hz)')\n",
    "#     plt.colorbar(format='%+2.0f dB')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be14c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #amplitude vs frequency\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# num_samples_to_visualize = 50\n",
    "\n",
    "# for i in range(num_samples_to_visualize):\n",
    "#     plt.figure(figsize=(7, 4))\n",
    "    \n",
    "#     fft_result = np.fft.fft(X[i])\n",
    "    \n",
    "#     frequencies = np.fft.fftfreq(len(fft_result), 1/sr)\n",
    "#     amplitudes = np.abs(fft_result)\n",
    "\n",
    "#     plt.plot(frequencies[:len(frequencies)//2], amplitudes[:len(amplitudes)//2])  \n",
    "#     filename = os.listdir(folder_path)[i]\n",
    "    \n",
    "#     plt.title(f'Amplitude vs. Frequency of {filename}')\n",
    "#     plt.xlabel('Frequency (Hz)')\n",
    "#     plt.ylabel('Amplitude')\n",
    "#     plt.grid()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258d260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # waveform plot\n",
    "# import librosa.display\n",
    "\n",
    "# num_samples_to_visualize = 40\n",
    "\n",
    "# for i in range(num_samples_to_visualize):\n",
    "#     plt.figure(figsize=(7, 4))\n",
    "#     plt.plot(np.arange(len(X[i])) / sr, X[i])\n",
    "#     filename = os.listdir(folder_path)[i]\n",
    "\n",
    "#     plt.title(f'Waveform of Audio Sample {filename}')\n",
    "#     plt.xlabel('Time (s)')\n",
    "#     plt.ylabel('Amplitude')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab2c5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64b1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Sample labels\n",
    "# y_train = [\"C\", \"Cs\", \"D\", \"Eb\", \"E\", \"F\", \"Fs\", \"G\", \"Gs\", \"A\", \"Bb\", \"B\"]\n",
    "\n",
    "# # Initialize the OneHotEncoder\n",
    "# one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# # Reshape and fit-transform the labels\n",
    "# y_train_encoded = one_hot_encoder.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "\n",
    "# # Now, y_train_encoded contains the one-hot encoded labels\n",
    "\n",
    "# # Inverse transformation (decoding)\n",
    "# y_train_decoded = one_hot_encoder.inverse_transform(y_train_encoded)\n",
    "\n",
    "# y_test_encoded = one_hot_encoder.transform(np.array(y_test).reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9692f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Define musical note names\n",
    "NOTE_NAMES = [\"C\", \"Cs\", \"D\", \"Eb\", \"E\", \"F\", \"Fs\", \"G\", \"Gs\", \"A\", \"Bb\", \"B\"]\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "# Define a learning rate schedule function\n",
    "def lr_schedule(epoch):\n",
    "    initial_learning_rate = 0.001  # Set your initial learning rate here\n",
    "    decay = 0.95  # Set the decay rate\n",
    "    if epoch > 5:  # Adjust this condition based on when you want to start reducing the learning rate\n",
    "        return initial_learning_rate * (decay ** (epoch - 5))\n",
    "    else:\n",
    "        return initial_learning_rate\n",
    "\n",
    "# Create a learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Create a neural network model\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y_train_encoded)), activation='softmax')  # Output layer with softmax for classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a9b6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (346, 44100)\n",
      "X_test shape: (87, 44100)\n",
      "y_train_encoded shape: (346,)\n",
      "y_test_encoded shape: (87,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes of your data\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train_encoded shape:\", y_train_encoded.shape)\n",
    "print(\"y_test_encoded shape:\", y_test_encoded.shape)\n",
    "\n",
    "# If there's an inconsistency, check your data split\n",
    "# Ensure that you are using the same random_state in train_test_split for reproducibility.\n",
    "# Make sure that X and y are correctly matched before the split.\n",
    "# Ensure that the test_size is reasonable and doesn't result in an extreme data split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea69df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "22/22 [==============================] - 4s 125ms/step - loss: 18.7551 - accuracy: 0.2312 - val_loss: 10.5844 - val_accuracy: 0.2069 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 2s 108ms/step - loss: 4.8085 - accuracy: 0.5000 - val_loss: 6.5497 - val_accuracy: 0.3908 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 2s 107ms/step - loss: 1.9248 - accuracy: 0.7023 - val_loss: 5.0767 - val_accuracy: 0.5517 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 1.3518 - accuracy: 0.8121 - val_loss: 3.2896 - val_accuracy: 0.7241 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 2s 110ms/step - loss: 0.9500 - accuracy: 0.8613 - val_loss: 3.4934 - val_accuracy: 0.7126 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 2s 113ms/step - loss: 0.4118 - accuracy: 0.9191 - val_loss: 3.2625 - val_accuracy: 0.7126 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 2s 106ms/step - loss: 0.3712 - accuracy: 0.9335 - val_loss: 2.9112 - val_accuracy: 0.6897 - lr: 9.5000e-04\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 2s 104ms/step - loss: 0.1311 - accuracy: 0.9711 - val_loss: 2.8712 - val_accuracy: 0.7471 - lr: 9.0250e-04\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 2s 69ms/step - loss: 0.3748 - accuracy: 0.9422 - val_loss: 4.4599 - val_accuracy: 0.6322 - lr: 8.5737e-04\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 2s 108ms/step - loss: 0.1413 - accuracy: 0.9595 - val_loss: 3.4850 - val_accuracy: 0.6782 - lr: 8.1451e-04\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 2s 102ms/step - loss: 0.1547 - accuracy: 0.9711 - val_loss: 3.0244 - val_accuracy: 0.7471 - lr: 7.7378e-04\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 2s 96ms/step - loss: 0.2272 - accuracy: 0.9480 - val_loss: 3.0867 - val_accuracy: 0.7126 - lr: 7.3509e-04\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 2s 108ms/step - loss: 0.2763 - accuracy: 0.9711 - val_loss: 2.5543 - val_accuracy: 0.7931 - lr: 6.9834e-04\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 2s 107ms/step - loss: 0.1824 - accuracy: 0.9624 - val_loss: 2.2416 - val_accuracy: 0.7586 - lr: 6.6342e-04\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 2s 101ms/step - loss: 0.1329 - accuracy: 0.9769 - val_loss: 2.6397 - val_accuracy: 0.7586 - lr: 6.3025e-04\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.0764 - accuracy: 0.9913 - val_loss: 2.4490 - val_accuracy: 0.7471 - lr: 5.9874e-04\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 2s 79ms/step - loss: 0.0369 - accuracy: 0.9913 - val_loss: 2.5676 - val_accuracy: 0.7356 - lr: 5.6880e-04\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 2s 75ms/step - loss: 0.0648 - accuracy: 0.9855 - val_loss: 2.3279 - val_accuracy: 0.7586 - lr: 5.4036e-04\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 2s 111ms/step - loss: 0.0794 - accuracy: 0.9855 - val_loss: 2.0331 - val_accuracy: 0.7586 - lr: 5.1334e-04\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 2s 98ms/step - loss: 0.0177 - accuracy: 0.9971 - val_loss: 2.3064 - val_accuracy: 0.7816 - lr: 4.8767e-04\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 2s 107ms/step - loss: 0.0723 - accuracy: 0.9971 - val_loss: 2.3767 - val_accuracy: 0.7701 - lr: 4.6329e-04\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 2s 103ms/step - loss: 0.1114 - accuracy: 0.9942 - val_loss: 2.6424 - val_accuracy: 0.7586 - lr: 4.4013e-04\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 3s 116ms/step - loss: 0.0423 - accuracy: 0.9971 - val_loss: 2.6078 - val_accuracy: 0.7816 - lr: 4.1812e-04\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 2s 99ms/step - loss: 0.0118 - accuracy: 0.9971 - val_loss: 2.6647 - val_accuracy: 0.7701 - lr: 3.9721e-04\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 2s 103ms/step - loss: 0.0174 - accuracy: 0.9971 - val_loss: 2.5503 - val_accuracy: 0.7586 - lr: 3.7735e-04\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 3s 117ms/step - loss: 0.0175 - accuracy: 0.9971 - val_loss: 2.5551 - val_accuracy: 0.7471 - lr: 3.5849e-04\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 3s 120ms/step - loss: 0.0073 - accuracy: 0.9971 - val_loss: 2.5193 - val_accuracy: 0.7586 - lr: 3.4056e-04\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 2s 95ms/step - loss: 0.0145 - accuracy: 0.9942 - val_loss: 2.4980 - val_accuracy: 0.7586 - lr: 3.2353e-04\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 3s 118ms/step - loss: 0.0157 - accuracy: 0.9942 - val_loss: 2.5117 - val_accuracy: 0.7586 - lr: 3.0736e-04\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 2s 111ms/step - loss: 0.0117 - accuracy: 0.9942 - val_loss: 2.5001 - val_accuracy: 0.7586 - lr: 2.9199e-04\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 2s 114ms/step - loss: 0.0110 - accuracy: 0.9942 - val_loss: 2.4873 - val_accuracy: 0.7586 - lr: 2.7739e-04\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 2s 103ms/step - loss: 0.0080 - accuracy: 0.9971 - val_loss: 2.5377 - val_accuracy: 0.7586 - lr: 2.6352e-04\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 2s 113ms/step - loss: 0.0140 - accuracy: 0.9971 - val_loss: 2.4815 - val_accuracy: 0.7586 - lr: 2.5034e-04\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 3s 115ms/step - loss: 0.0087 - accuracy: 0.9971 - val_loss: 2.5083 - val_accuracy: 0.7586 - lr: 2.3783e-04\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 2s 104ms/step - loss: 0.0109 - accuracy: 0.9971 - val_loss: 2.4725 - val_accuracy: 0.7586 - lr: 2.2594e-04\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 2s 110ms/step - loss: 0.0096 - accuracy: 0.9942 - val_loss: 2.4587 - val_accuracy: 0.7586 - lr: 2.1464e-04\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 2.4609 - val_accuracy: 0.7586 - lr: 2.0391e-04\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 2s 112ms/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 2.4837 - val_accuracy: 0.7586 - lr: 1.9371e-04\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 2s 104ms/step - loss: 0.0066 - accuracy: 0.9971 - val_loss: 2.4683 - val_accuracy: 0.7586 - lr: 1.8403e-04\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 3s 123ms/step - loss: 0.0071 - accuracy: 0.9942 - val_loss: 2.4761 - val_accuracy: 0.7586 - lr: 1.7482e-04\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 2s 102ms/step - loss: 0.0081 - accuracy: 0.9971 - val_loss: 2.4355 - val_accuracy: 0.7586 - lr: 1.6608e-04\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 2s 107ms/step - loss: 0.0085 - accuracy: 0.9971 - val_loss: 2.4644 - val_accuracy: 0.7586 - lr: 1.5778e-04\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 2s 101ms/step - loss: 0.0078 - accuracy: 0.9942 - val_loss: 2.4717 - val_accuracy: 0.7586 - lr: 1.4989e-04\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 2s 93ms/step - loss: 0.0112 - accuracy: 0.9942 - val_loss: 2.4500 - val_accuracy: 0.7586 - lr: 1.4240e-04\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 2s 99ms/step - loss: 0.0086 - accuracy: 0.9971 - val_loss: 2.4575 - val_accuracy: 0.7586 - lr: 1.3528e-04\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.0065 - accuracy: 0.9942 - val_loss: 2.4389 - val_accuracy: 0.7586 - lr: 1.2851e-04\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 2s 114ms/step - loss: 0.0070 - accuracy: 0.9971 - val_loss: 2.4559 - val_accuracy: 0.7586 - lr: 1.2209e-04\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 2s 109ms/step - loss: 0.0069 - accuracy: 0.9942 - val_loss: 2.4483 - val_accuracy: 0.7586 - lr: 1.1598e-04\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 2s 77ms/step - loss: 0.0078 - accuracy: 0.9971 - val_loss: 2.4445 - val_accuracy: 0.7586 - lr: 1.1018e-04\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 2s 101ms/step - loss: 0.0069 - accuracy: 0.9942 - val_loss: 2.4419 - val_accuracy: 0.7586 - lr: 1.0467e-04\n"
     ]
    }
   ],
   "source": [
    "# Train your model using X_train_augmented\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train_encoded, epochs=50, batch_size=16,\n",
    "                    validation_data=(X_test, y_test_encoded),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e1ce19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 34ms/step - loss: 2.4419 - accuracy: 0.7586\n",
      "Test Loss: 2.4419, Test Accuracy: 0.7586\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using encoded labels\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3a85fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ea151d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install matplotlib scipy plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4ff9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_fft(p, xf, fs, notes):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(\"Frequency Spectrum\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.plot(xf, p)\n",
    "\n",
    "    for note in notes:\n",
    "        plt.annotate(note[1], (note[0] + 10, note[2]), fontsize=12, ha='center', va='bottom')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def extract_sample(audio, frame_number, frame_offset):\n",
    "    end = frame_number * frame_offset\n",
    "    begin = int(end - FFT_WINDOW_SIZE)\n",
    "\n",
    "    if end == 0:\n",
    "        return np.zeros((np.abs(begin)), dtype=float)\n",
    "    elif begin < 0:\n",
    "        return np.concatenate([np.zeros((np.abs(begin)), dtype=float), audio[0:end]])\n",
    "    else:\n",
    "        return audio[begin:end]\n",
    "\n",
    "def find_top_notes(fft, num, xf):\n",
    "    if np.max(fft.real) < 0.001:\n",
    "        return []\n",
    "\n",
    "    lst = [x for x in enumerate(fft.real)]\n",
    "    lst = sorted(lst, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    idx = 0\n",
    "    found = []\n",
    "    found_note = set()\n",
    "    while (idx < len(lst)) and (len(found) < num):\n",
    "        f = xf[lst[idx][0]]\n",
    "        y = lst[idx][1]\n",
    "        n = freq_to_number(f)\n",
    "        n0 = int(round(n))\n",
    "        name = note_name(n0)\n",
    "\n",
    "        if name not in found_note:\n",
    "            found_note.add(name)\n",
    "            s = [f, note_name(n0), y]\n",
    "            found.append(s)\n",
    "        idx += 1\n",
    "\n",
    "    return found\n",
    "\n",
    "def freq_to_number(f): return 69 + 12 * np.log2(f / 440.0)\n",
    "def note_name(n): return NOTE_NAMES[n % 12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c072d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_fft(magnitudes, sample_rate, notes, xlim=None):\n",
    "#     fft_values = np.fft.fftfreq(len(magnitudes), 1.0 / sample_rate)\n",
    "# #\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.title(\"Frequency Spectrum\")\n",
    "#     plt.xlabel(\"Frequency (Hz)\")\n",
    "#     plt.ylabel(\"Magnitude\")\n",
    "#     plt.plot(fft_values, magnitudes)\n",
    "    \n",
    "#     if xlim:\n",
    "#         plt.xlim(xlim)\n",
    "    \n",
    "#     for note in notes:\n",
    "#         plt.annotate(note[1], (note[0] + 10, note[2]), fontsize=12, ha='center', va='bottom')\n",
    "    \n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "\n",
    "# # Example: Replace this with your actual audio_samples and sample_rate\n",
    "# # Generate random audio data for demonstration purposes\n",
    "# sample_rate = 44100\n",
    "# duration = 5  # 5 seconds of audio\n",
    "# num_samples = int(sample_rate * duration)\n",
    "# audio_samples = np.random.randn(num_samples)\n",
    "\n",
    "# # Extract features and notes (replace with your actual audio and notes)\n",
    "# features = extract_features([audio_samples])\n",
    "# num_notes = 5  # Adjust the number of top notes you want to display\n",
    "\n",
    "# for feature in features:\n",
    "#     notes = find_top_notes(feature, num_notes, fft_values)  # Replace with your notes\n",
    "#     plot_fft(feature, sample_rate, notes, xlim=(0, 1000))  # Adjust frequency range if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d186a315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trial1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trial1\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"trial1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2302c10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020854D9F9C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020854D9F9C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 129ms/step\n",
      "Predicted Note: Bb\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define musical note names\n",
    "NOTE_NAMES = [\"C\", \"Cs\", \"D\", \"Eb\", \"E\", \"F\", \"Fs\", \"G\", \"Gs\", \"A\", \"Bb\", \"B\"]\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model('trial1')  # Replace 'your_model_path' with the actual path to your trained model\n",
    "\n",
    "def extract_features(audio):\n",
    "    # Applying FFT\n",
    "    fft_result = np.fft.fft(audio)\n",
    "    magnitudes = np.abs(fft_result)\n",
    "    return magnitudes\n",
    "\n",
    "def predict_note(audio_file_path, model):\n",
    "    # Load and process the audio file\n",
    "    audio, _ = librosa.load(audio_file_path, sr=None)\n",
    "    features = extract_features(audio)\n",
    "    \n",
    "    # Make sure the features have the same shape as the model's input shape\n",
    "    if len(features) < X_train.shape[1]:\n",
    "        features = np.pad(features, (0, X_train.shape[1] - len(features)))\n",
    "    elif len(features) > X_train.shape[1]:\n",
    "        features = features[:X_train.shape[1]]\n",
    "    \n",
    "    features = np.array([features])  # Reshape for model input\n",
    "    \n",
    "    # Make a prediction using the trained model\n",
    "    prediction = model.predict(features)\n",
    "    \n",
    "    # Convert prediction to musical note\n",
    "    predicted_note_index = np.argmax(prediction)\n",
    "    predicted_note = NOTE_NAMES[predicted_note_index]\n",
    "    \n",
    "    return predicted_note\n",
    "\n",
    "# Example usage:\n",
    "audio_file_path = 'piano_triads/C_maj_6_0.wav'  # Replace 'your_audio_file_path.wav' with the path to the user's input audio file\n",
    "predicted_note = predict_note(audio_file_path, model)\n",
    "print(f\"Predicted Note: {predicted_note}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa826e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61add361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
